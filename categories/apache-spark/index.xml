<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Apache Spark on Data Programmers</title>
    <link>https://www.datarchy.tech/categories/apache-spark/</link>
    <description>Recent content in Apache Spark on Data Programmers</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 27 Aug 2020 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="https://www.datarchy.tech/categories/apache-spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to optimise loading partitioned JSON data in Spark ?</title>
      <link>https://www.datarchy.tech/code/20200827_load_partitioned_json/</link>
      <pubDate>Thu, 27 Aug 2020 00:00:00 +0100</pubDate>
      
      <guid>https://www.datarchy.tech/code/20200827_load_partitioned_json/</guid>
      <description>In this tutorial we will explore ways to optimise loading partitioned JSON data in Spark.
I have used the SF Bay Area Bike Share dataset, you can find it here. The original data (status.csv) have gone through few transformations. The result looks like:
Loading from partitioned JSON files  We will load the data filtered by station and month :
val df1 = spark.read .json(&amp;#34;file:///data/bike-data-big/partitioned_status.json&amp;#34;) .filter(&amp;#34;station_id = 10 and (month in (&amp;#39;2013-08&amp;#39;, &amp;#39;2013-09&amp;#39;))&amp;#34;) Despite the fact that the code above does not contain any action yet, Spark starts three jobs that took few minutes to complete (on a local setting, with 8 cores and 32 Gigs or RAM):  Job 0 and Job 1 : Spark (and more specifically the Data Source API) is listing the content of the root folder and its subfolders, in order to discover the partitioning columns and map each column value to a path.</description>
    </item>
    
    <item>
      <title>How to add row numbers to a Spark DataFrame?</title>
      <link>https://www.datarchy.tech/code/20200820_spark_rows_number/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0100</pubDate>
      
      <guid>https://www.datarchy.tech/code/20200820_spark_rows_number/</guid>
      <description>In this tutorial, we will explore a couple of ways to add a sequential consecutive row number to a dataframe.
For example, let this be our dataframe (taken from Spark: The Definitive Guide github repo):
val df = spark.read.option(&amp;#34;header&amp;#34;, &amp;#34;true&amp;#34;).csv(&amp;#34;.../data/flight-data/csv/*&amp;#34;) +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ only showing top 5 rows After adding a column containing the row number, the result should look like:</description>
    </item>
    
    <item>
      <title>Spark DataFrame - two ways to count the number of rows per partition</title>
      <link>https://www.datarchy.tech/code/20200814_spark_count_per_partition/</link>
      <pubDate>Sat, 15 Aug 2020 00:00:00 +0100</pubDate>
      
      <guid>https://www.datarchy.tech/code/20200814_spark_count_per_partition/</guid>
      <description>Sometimes, we are required to compute the number of rows per each partition. To do this, there are two ways:
  The first way is using Dataframe.mapPartitions().
  The second way (the faster according to my observations) is using the spark_partition_id() function, followed by a grouping count aggregation.
  Let&amp;rsquo;s first load the data into a dataframe I have used the SF Bay Area Bike data source, that can be found here</description>
    </item>
    
  </channel>
</rss>